{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1608071770853,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "iwUUrgKDOZIe",
    "outputId": "94b4d294-31eb-4cd7-f198-b715933e69ab"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-91874b305a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjdpJuG6eH3f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lnXLfgAeINo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1608066487085,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "XS0WaHpBOZIj",
    "outputId": "cb6a3d52-75a7-4fd9-edbb-c1b50fba1d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/Caption-Translation\n"
     ]
    }
   ],
   "source": [
    "cd drive/My\\ Drive/Colab\\ Notebooks/Caption-Translation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here if not using Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1608071774444,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "L-yPyQtbPDw_",
    "outputId": "04254386-f430-463a-90bc-60048e244746"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gautam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1608076176973,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "7zRaiu9AOZIk"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from matplotlib import pyplot\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ssl\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CaptionScore:\n",
    "    model = None\n",
    "    verbose = False\n",
    "\n",
    "    def __init__(self, num_words = 50000):\n",
    "        self.model = KeyedVectors.load_word2vec_format(\"word2vec_pretrained.txt\", limit=num_words, binary=False)\n",
    "\n",
    "\n",
    "    def get_score(self, candidate, objects): # candidate is string caption; objects is the list of objects\n",
    "        score = 0\n",
    "        for objectId in objects:\n",
    "            if CLASSES[objectId]==\"N/A\":\n",
    "                continue\n",
    "            tokens = word_tokenize(candidate)\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "            if(CLASSES[objectId] in tokens):\n",
    "                score += 1\n",
    "            else:\n",
    "                max_sim = 0\n",
    "                max_token = None\n",
    "                for token in tokens:\n",
    "                    if(len(token) > 3 or token.isalpha()):\n",
    "                        new_sim = 0\n",
    "                        try:\n",
    "                            new_sim = model.model.similarity(token, CLASSES[objectId])\n",
    "                        except:\n",
    "                            pass\n",
    "                        if (new_sim > max_sim):\n",
    "                            max_sim = new_sim\n",
    "                            max_token = token\n",
    "                if self.verbose:\n",
    "                      print(\"Fitted object '\", CLASSES[objectId], \"' to '\", max_token, \"' with similarity\", max_sim)\n",
    "                score += max_sim\n",
    "        return score\n",
    "    \n",
    "    def get_score_relu(self, candidate, objects): # candidate is string caption; objects is the list of objects\n",
    "        score = 0\n",
    "        for objectId in objects:\n",
    "            if CLASSES[objectId]==\"N/A\":\n",
    "                continue\n",
    "            tokens = word_tokenize(candidate)\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "            if(CLASSES[objectId] in tokens):\n",
    "                score += 1\n",
    "            else:\n",
    "                sum_sim = 0\n",
    "                for token in tokens:\n",
    "                    if(len(token) > 3 or token.isalpha()):\n",
    "                        new_sim = 0\n",
    "                        try:\n",
    "                            new_sim = model.model.similarity(token, CLASSES[objectId])\n",
    "                            sum_sim += max(0.6, new_sim)\n",
    "                        except:\n",
    "                            pass\n",
    "                if self.verbose:\n",
    "                    print(\"Fitted object '\", CLASSES[objectId], \"' to '\", max_token, \"' with similarity\", max_sim)\n",
    "                score += sum_sim / len(tokens)\n",
    "        return score\n",
    "    \n",
    "    def get_score_arctanh(self, candidate, objects):\n",
    "        score = 0\n",
    "        for objectId in objects:\n",
    "            if CLASSES[objectId]==\"N/A\":\n",
    "                continue\n",
    "            tokens = word_tokenize(candidate)\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "            if(CLASSES[objectId] in tokens):\n",
    "                score += 1\n",
    "            else:\n",
    "                sum_sim = 0\n",
    "                for token in tokens:\n",
    "                    if(len(token) > 3 or token.isalpha()):\n",
    "                        new_sim = 0\n",
    "                        try:\n",
    "                            new_sim = model.model.similarity(token, CLASSES[objectId])\n",
    "                            sum_sim += np.arctanh(new_sim)\n",
    "                        except:\n",
    "                            pass\n",
    "                if self.verbose:\n",
    "                    print(\"Fitted object '\", CLASSES[objectId], \"' to '\", max_token, \"' with similarity\", max_sim)\n",
    "                score += sum_sim / len(tokens)\n",
    "        return score\n",
    "    \n",
    "    def get_score_average(self, candidate, objects):\n",
    "        score = 0\n",
    "        for objectId in objects:\n",
    "            if CLASSES[objectId]==\"N/A\":\n",
    "                continue\n",
    "            tokens = word_tokenize(candidate)\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "            if(CLASSES[objectId] in tokens):\n",
    "                score += 1\n",
    "            else:\n",
    "                sum_sim = 0\n",
    "                for token in tokens:\n",
    "                    if(len(token) > 3 or token.isalpha()):\n",
    "                        new_sim = 0\n",
    "                        try:\n",
    "                            new_sim = model.model.similarity(token, CLASSES[objectId])\n",
    "                            sum_sim += new_sim\n",
    "                        except:\n",
    "                            pass\n",
    "                if self.verbose:\n",
    "                    print(\"Fitted object '\", CLASSES[objectId], \"' to '\", max_token, \"' with similarity\", max_sim)\n",
    "                score += sum_sim / len(tokens)\n",
    "        return score\n",
    "    \n",
    "    def get_score_rms(self, candidate, objects):\n",
    "        score = 0\n",
    "        for objectId in objects:\n",
    "            if CLASSES[objectId]==\"N/A\":\n",
    "                continue\n",
    "            tokens = word_tokenize(candidate)\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "            if(CLASSES[objectId] in tokens):\n",
    "                score += 1\n",
    "            else:\n",
    "                sum_sim = 0\n",
    "                for token in tokens:\n",
    "                    if(len(token) > 3 or token.isalpha()):\n",
    "                        new_sim = 0\n",
    "                        try:\n",
    "                            new_sim = model.model.similarity(token, CLASSES[objectId])\n",
    "                            sum_sim += new_sim ** 2\n",
    "                        except:\n",
    "                            pass\n",
    "                if self.verbose:\n",
    "                    print(\"Fitted object '\", CLASSES[objectId], \"' to '\", max_token, \"' with similarity\", max_sim)\n",
    "                score += (sum_sim / len(tokens)) ** 0.5\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 13990,
     "status": "ok",
     "timestamp": 1608076193951,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "QxVPa2B_OZIk"
   },
   "outputs": [],
   "source": [
    "model = CaptionScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1608076197286,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "_7XReJDXl1Hx",
    "outputId": "5cd080f8-f8de-4737-ebb4-58d5e3e95d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3807391\n"
     ]
    }
   ],
   "source": [
    "print(model.model.similarity('person', 'the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5798,
     "status": "ok",
     "timestamp": 1608076676245,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "y4sHmza0OZIk",
    "outputId": "9f7cd4ea-610d-4692-9266-1c5c99d7417c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import sacrebleu\n",
    "\n",
    "x = json.load(open('data.json', 'r'))\n",
    "\n",
    "selectedCaptions = []\n",
    "refs = []\n",
    "\n",
    "for (index, row) in enumerate(x):\n",
    "    if(index % 1000 == 0):\n",
    "        print(index)\n",
    "    \n",
    "    if row['objects'][0] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        ibm_caption = row['translations']['ibm']\n",
    "        azure_caption = row['translations']['azure']\n",
    "\n",
    "        ibm_score = model.get_score_relu(ibm_caption, set(row['objects']))\n",
    "        azr_score = model.get_score_relu(azure_caption, set(row['objects']))\n",
    "\n",
    "        r = {'ibm': row['translations']['ibm'], 'azure': row['translations']['azure']}\n",
    "\n",
    "    #     if(ibm_score != azr_score):\n",
    "            #print(row)\n",
    "\n",
    "        if ibm_score > azr_score:\n",
    "            r['selected'] = 'ibm'\n",
    "        elif ibm_score == azr_score:\n",
    "            r['selected'] = 'ibm'\n",
    "        else:\n",
    "            r['selected'] = 'azure'\n",
    "\n",
    "        ref = row['caption_en']\n",
    "        refs.append(ref)\n",
    "\n",
    "        azureScoreBleu = sacrebleu.corpus_bleu([azure_caption], [[ref]])\n",
    "        ibmScoreBleu = sacrebleu.corpus_bleu([ibm_caption], [[ref]])\n",
    "\n",
    "        if ibmScoreBleu.score > azureScoreBleu.score:\n",
    "            r['bleuSelected'] = 'ibm'\n",
    "        elif ibmScoreBleu.score == azureScoreBleu.score:\n",
    "            r['bleuSelected'] = 'ibm'\n",
    "        else:\n",
    "            r['bleuSelected'] = 'azure'\n",
    "\n",
    "        selectedCaptions.append(r)\n",
    "    \n",
    "    \n",
    "    # else:\n",
    "    #   selectedCaptions.append(random.choice(['ibm', 'azr']))\n",
    "    #print('')\n",
    "    #print(row['objects'])\n",
    "    #print('Image:', row['filename'], 'IBM:', model.get_score(row['translations']['ibm'], set(row['objects'])), 'AZURE:', model.get_score(row['translations']['azure'], set(row['objects'])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1608076686160,
     "user": {
      "displayName": "Shaya Zarkesh",
      "photoUrl": "",
      "userId": "05727552256173935167"
     },
     "user_tz": 480
    },
    "id": "2pApOMLnOZIl",
    "outputId": "73fcd954-5365-4fbf-f01d-1e8885032c56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure: 32.41654720349349\n",
      "IBM: 33.535709688035226\n",
      "Combined: 32.40535495432709\n",
      "Correct Selections: 37.60152861508464\n"
     ]
    }
   ],
   "source": [
    "azureBleu = sacrebleu.corpus_bleu([a['azure'] for a in selectedCaptions], [refs])\n",
    "print(\"Azure: \" + str(azureBleu.score))\n",
    "\n",
    "ibmBleu = sacrebleu.corpus_bleu([a['ibm'] for a in selectedCaptions], [refs])\n",
    "print(\"IBM: \" + str(ibmBleu.score))\n",
    "\n",
    "selectionBleu = sacrebleu.corpus_bleu([a[a['selected']] for a in selectedCaptions], [refs])\n",
    "print(\"Combined: \" + str(selectionBleu.score))\n",
    "\n",
    "correctSelectionBleu = sacrebleu.corpus_bleu([a[a['bleuSelected']] for a in selectedCaptions], [refs])\n",
    "print(\"Correct Selections: \" + str(correctSelectionBleu.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "evaluate_translations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
